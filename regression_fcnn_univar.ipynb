{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "regression_fcnn_univar.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzyZt3jsz0st"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import random\n",
        "import math\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "XthC_Jr40waf"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# %matplotlib inline\n",
        "\n",
        "plt.style.use('ggplot')\n",
        "plt.rcParams['figure.figsize'] = (10, 10)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "File=\"/content/drive/MyDrive/Assignment 1 Data/Regression/UnivariateData/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjqJLji_05Wr",
        "outputId": "a21403af-5d43-4e4d-ca99-43cc122a09ad"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def readData(file):\n",
        "    data=pd.read_csv(file+\".txt\",\",\",header=None)\n",
        "    data=np.array(data.values)\n",
        "    return data\n",
        "    \n",
        "Folder=[File+\"32\"]\n",
        "def train_test_split(arr, test):\n",
        "    y=np.full(len(arr),1)\n",
        "    \n",
        "    train= np.array([y[:-int(len(arr)*(test))],arr[:-int(len(arr)*(test)),0],arr[:-int(len(arr)*(test)),1]])\n",
        "   \n",
        "    test=np.array([y[-int(len(arr)*test): ],arr[-int(len(arr)*test): ,0],arr[-int(len(arr)*test): ,1]])\n",
        "    \n",
        "    return train.T, test.T\n",
        "\n",
        "for k in Folder:\n",
        "        \n",
        "    folder=k\n",
        "    data =  readData(k)\n",
        "    train_data, Test=  train_test_split(data,0.20)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "Np55OCy_06B7",
        "outputId": "29f7fe89-c38b-42c3-fb5a-155a734ff507"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-5b5fc21e4021>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mfolder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mreadData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTest\u001b[0m\u001b[0;34m=\u001b[0m  \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-5b5fc21e4021>\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(arr, test)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 1 with size 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# Decision boundary\n",
        "def generate_values_boundary2(data,weights,archicture,ttl):\n",
        "   \n",
        "\n",
        "    \n",
        "    network_outputs=[]\n",
        "    for i in range(len(data)):\n",
        "\n",
        "        input_data=np.array(data[i][:2])\n",
        "#         network_outputs.append(input_data) \n",
        "#         print(input_data)\n",
        "        #Calculation of the weighted sum at the every layer of the neural network\n",
        "        for h in range(len(archicture)-1):\n",
        "\n",
        "            if h== len(archicture)-2:\n",
        "                output_vector=np.dot(weights[h],input_data)\n",
        "#                 print(output_vector)\n",
        "            else:\n",
        "                output_vector=perceptron(weights[h],input_data,sigmoid)\n",
        "#                 print(output_vector)\n",
        "            input_data=np.concatenate(([1],output_vector))\n",
        "            \n",
        "        network_outputs.append(output_vector[0])\n",
        "\n",
        "\n",
        "    z= np.array(network_outputs)\n",
        "#     print(data[:,3])  \n",
        "#     print(z)\n",
        "    plt.scatter(data[:,2],z,marker='D',color='#EAA358',edgecolors='black')\n",
        "    \n",
        "    \n",
        "    \n",
        "    print(ttl)\n",
        "    plt.xlabel(\"Target Output\")  # add X-axis label \n",
        "    plt.ylabel(\"Model Output\") \n",
        "#     plt.savefig(ttl+'.png',bbox_inches = 'tight')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "84QtfJ4m06MG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Decision boundary\n",
        "def generate_values_boundary(data,archicture,weights,sigmoid,ttl):\n",
        "    min_x=min(data[:,1])\n",
        "    max_x=max(data[:,1])\n",
        "    x_mesh = np.linspace(min_x, max_x, 500)\n",
        "\n",
        "    y_pred=[]\n",
        "    for i in range(len(x_mesh)):\n",
        "        input_data=np.copy(np.array([1,x_mesh[i]]))\n",
        "        for h in range(len(archicture)-1):\n",
        "\n",
        "            if h== len(archicture)-2:\n",
        "                output_vector=np.dot(weights[h],input_data)\n",
        "            else:\n",
        "                output_vector=perceptron(weights[h],input_data,sigmoid)\n",
        "                \n",
        "            input_data=np.concatenate(([1],output_vector))\n",
        "            \n",
        "        y_pred.append(output_vector)\n",
        "   \n",
        "    \n",
        "    plt.plot(x_mesh,y_pred,color='b',label='p1')\n",
        "    \n",
        "                    \n",
        "    plt.scatter(data[:,1],data[:,2],marker='D',color='#EAA358',edgecolors='black')\n",
        "    print(ttl)\n",
        "    plt.xlabel(\"x1\")  # add X-axis label \n",
        "    plt.ylabel(\"x2\") \n",
        "    plt.savefig(ttl+'.png',bbox_inches = 'tight')\n",
        "    plt.show()\n",
        "\n",
        "plt.scatter(data[:,0],data[:,1],marker='D',color='#EAA358',edgecolors='black')\n",
        "plt.xlabel(\"x1\")  # add X-axis label \n",
        "plt.ylabel(\"x2\") \n",
        "plt.savefig('data.png',bbox_inches = 'tight')\n",
        "\n",
        "def logistic(input):\n",
        "    x=np.array(input,dtype=np.float128)\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "def perceptron(weights,input,sigmoid):\n",
        "    \n",
        "    predict=np.dot(weights,input)\n",
        "    \n",
        "    if(sigmoid==1):\n",
        "        predict=np.tanh(predict)\n",
        "        \n",
        "    elif sigmoid==2:\n",
        "        return predict\n",
        "        \n",
        "    else:\n",
        "        predict=logistic(predict)\n",
        "        \n",
        "    return predict\n",
        "\n",
        "def reg_derivative(x,sigmoid):\n",
        " \n",
        "    return 1\n",
        "\n",
        "def activation_derivative(x,sigmoid):\n",
        "    \n",
        "    if sigmoid:\n",
        "        t=(np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
        "        dt=1-t**2\n",
        "    \n",
        "    else:\n",
        "        dt= x * (1.0 - x)\n",
        "        \n",
        "    return dt\n",
        "\n",
        "def calculate_delta(a,b):\n",
        "    \n",
        "    c=np.zeros(a.shape)\n",
        "    for i in range(a.shape[0]):\n",
        "        for j in range(a.shape[1]):\n",
        "\n",
        "            if j!=0:\n",
        "                c[i][j]=a[i][j]*b[i]\n",
        "    \n",
        "    return np.sum(c,axis=0)[1:]\n",
        "\n",
        "def network(data,archicture,epoch,sigmoid,learning_rate):\n",
        "    \n",
        "    weights=[]\n",
        "\n",
        "    for h in range(len(archicture)-1):\n",
        "        weight=np.array([[random.seed(i+j) or random.random() for i in range(archicture[h]+1)] for j in range(archicture[h+1])])\n",
        "        weights.append(weight)\n",
        "        \n",
        "    Error_epoch=list()\n",
        "    \n",
        "    \n",
        "    for j in range(epoch):\n",
        "        \n",
        "\n",
        "        output=list()\n",
        "        expected=list()\n",
        "        np.random.shuffle(data)\n",
        "        \n",
        "        for i in range(len(data)):\n",
        "            \n",
        "            input_data=np.copy(data[i][:2])\n",
        "            print(input_data)\n",
        "        \n",
        "            network_outputs=[]\n",
        "            network_outputs.append(input_data) \n",
        "            \n",
        "            #Calculation of the weighted sum at the every layer of the neural network\n",
        "            for h in range(len(archicture)-1):\n",
        "                \n",
        "                if h== len(archicture)-2:\n",
        "                    output_vector=np.dot(weights[h],input_data)\n",
        "                else:\n",
        "                    output_vector=perceptron(weights[h],input_data,sigmoid)\n",
        "                    \n",
        "                input_data=np.concatenate(([1],output_vector))\n",
        "                network_outputs.append(input_data) \n",
        "#             print('output',input_data)\n",
        "           \n",
        "            #Send the data for backpropogation \n",
        "            target=np.copy(data[i][2])  \n",
        "            weights=back_propagate(archicture,target,network_outputs,weights, learning_rate,sigmoid) \n",
        "            output.append(network_outputs[-1][1:])\n",
        "            expected.append(list(data[i][2:]))\n",
        "            \n",
        "            \n",
        "        #Calculate the MSE for the every epoch  \n",
        "        error_output=np.array(expected)-np.array(output)\n",
        "        cost=(np.sum(error_output*error_output,axis=0)/len(error_output))\n",
        "        Etotal=np.sum(cost)\n",
        "        Error_epoch.append(Etotal)\n",
        "        \n",
        "        #print the error of every output neuron in every 100 epochs\n",
        "#         if j%100==0:\n",
        "#             print('Epoch',j,': ',cost)\n",
        "\n",
        "        \n",
        "    \n",
        "    return Error_epoch,weights\n"
      ],
      "metadata": {
        "id": "9mNV47e706cy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def back_propagate(archicture,target,network_outputs,weights, learning_rate,sigmoid):\n",
        "    \n",
        "    Delta=[]\n",
        "    length=len(archicture)\n",
        "    \n",
        "    #Calculate the delta for the final output layer\n",
        "    Do=-2*(target-network_outputs[-1:][0][1:])*reg_derivative(1,sigmoid)\n",
        "    Delta.append(Do)\n",
        "    \n",
        "    #Claculate the delta for all internal layers\n",
        "    for h in reversed (range(1,length-1)):\n",
        "        new_Delta=calculate_delta(weights[h],Delta[-1])\n",
        "        new_Delta=new_Delta*activation_derivative(network_outputs[h-length][1:],sigmoid)\n",
        "        Delta.append(new_Delta)\n",
        "    Delta.reverse()\n",
        "    \n",
        "    #Calculate the new updated value for the weights   \n",
        "    \n",
        "    k=-1\n",
        "    for W in weights:\n",
        "        k=k+1\n",
        "        for i in range(W.shape[0]):\n",
        "            for j in range(W.shape[1]):\n",
        "                W[i][j]=W[i][j]-learning_rate*network_outputs[k][j]*Delta[k][i]\n",
        "        \n",
        "    return weights\n"
      ],
      "metadata": {
        "id": "0Y6AvNcB060q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def classifier(data,archicture,epoch,sigmoid,learning_rate,file):\n",
        "    \n",
        "    error_output,weights=network(np.copy(data),archicture,epoch,sigmoid,learning_rate)\n",
        "    \n",
        "    plt.plot(error_output)\n",
        "    plt.xlabel(\"Epochs\")  # add X-axis label \n",
        "    plt.ylabel(\"Error\")  # add Y-axis label \n",
        "    plt.title(\"Error Plot\")  # add title \n",
        "    #plt.savefig(file+'Figure/Uni_Error.png',bbox_inches = 'tight')\n",
        "\n",
        "\n",
        "    \n",
        "    return weights\n",
        "\n",
        "def test1(data,weights,archicture,sigmoid):\n",
        "    \n",
        "\n",
        "        output=list()\n",
        "        neu_output=list()\n",
        "        expected=list()\n",
        "        np.random.shuffle(data)\n",
        "        \n",
        "        for i in range(len(data)):\n",
        "            \n",
        "            input_data=np.copy(data[i][:2])\n",
        "            print(input_data)\n",
        "        \n",
        "            network_outputs=[]\n",
        "            network_outputs.append(input_data) \n",
        "            \n",
        "            #Calculation of the weighted sum at the every layer of the neural network\n",
        "            for h in range(len(archicture)-1):\n",
        "                \n",
        "                if h== len(archicture)-2:\n",
        "                    output_vector=np.dot(weights[h],input_data)\n",
        "                else:\n",
        "                    output_vector=perceptron(weights[h],input_data,sigmoid)\n",
        "                    neu_output.append(output_vector)\n",
        "                    \n",
        "                input_data=np.concatenate(([1],output_vector))\n",
        "                network_outputs.append(input_data) \n",
        "#             print('output',input_data)\n",
        "           \n",
        "            #Send the data for backpropogation \n",
        "            target=np.copy(data[i][2])  \n",
        "            back_propagate(archicture,target,network_outputs,weights, learning_rate,sigmoid)\n",
        "            output.append(network_outputs[-1][1:])\n",
        "            expected.append(list(data[i][3:]))\n",
        "            \n",
        "            \n",
        "        #Calculate the MSE for the every epoch  \n",
        "        error_output=np.array(expected)-np.array(output)\n",
        "        cost=(np.sum(error_output*error_output,axis=0)/len(error_output))\n",
        "        Etotal=np.sum(cost)\n",
        "        \n",
        "       \n",
        "\n",
        "        \n",
        "    \n",
        "        return Etotal,neu_output\n",
        "\n",
        "def cross_validation(k,Arr):\n",
        "\n",
        "\n",
        "\n",
        "    length1=int(len(Arr)*0.25)\n",
        "\n",
        "    \n",
        "    M1,M2,M3,M4=Arr[:length1].copy(),Arr[length1:2*length1],Arr[2*length1:3*length1],Arr[3*length1:4*length1]\n",
        "   \n",
        "    if k==1:\n",
        "        M=np.concatenate((M2,M3,M4))\n",
        "        V=M1\n",
        "    if k==2:\n",
        "        M=np.concatenate((M1,M3,M4))\n",
        "        V=M2\n",
        "    if k==3:\n",
        "        M=np.concatenate((M1,M2,M4))\n",
        "        V=M3\n",
        "    if k==4:\n",
        "        M=np.concatenate((M1,M2,M3))\n",
        "        V=M4\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "    return M, V\n",
        "\n",
        "#archicture[no_of_input,no_of_neuron_h1,no_of_neuron_h2,no_of_output]\n",
        "#archicture[no_of_input,no_of_neuron_h1,no_of_output]\n",
        "#sigmoid=0 means logistic and sigmoid=1 means tanh\n",
        "\n",
        "archicture=[1,3,1]\n",
        "sigmoid=0\n",
        "learning_rate=0.1\n",
        "epoch=50\n",
        "k=3 # k=1, 2, 3, 4 for the cross validation, where 1 is Fold-1\n",
        "plt.rcParams['figure.figsize'] = (10, 10)\n",
        "\n",
        "\n",
        "\n",
        "Train,Validation=cross_validation(k,train_data)\n",
        "Weights=classifier(train_data,archicture,epoch,sigmoid,learning_rate,File)\n",
        "\n",
        "generate_values_boundary(Train,archicture,Weights,sigmoid,'Training Data')\n",
        "\n",
        "def test(data,weights,archicture,sigmoid):\n",
        "    \n",
        "        output=[]\n",
        "        expected=[]\n",
        "        neu_output=list()\n",
        "        for i in range(len(data)):\n",
        "            \n",
        "            input_data=np.copy(data[i][:2])\n",
        "        \n",
        "            network_outputs=[]\n",
        "            network_outputs.append(input_data) \n",
        "            \n",
        "            #Calculation of the weighted sum at the every layer of the neural network\n",
        "            for h in range(len(archicture)-1):\n",
        "                \n",
        "                if h== len(archicture)-2:\n",
        "                    output_vector=np.dot(weights[h],input_data)\n",
        "#                     neu_output.append(output_vector)\n",
        "                else:\n",
        "                    output_vector=perceptron(weights[h],input_data,sigmoid)\n",
        "                    neu_output.append(output_vector)\n",
        "                    \n",
        "                input_data=np.concatenate(([1],output_vector))\n",
        "                network_outputs.append(input_data) \n",
        "#             print('output',input_data)\n",
        "           \n",
        "            #Send the data for backpropogation \n",
        "            target=np.copy(data[i][2])  \n",
        "            output.append(network_outputs[-1][1:])\n",
        "            expected.append(list(data[i][2:]))\n",
        "            \n",
        "            \n",
        "        #Calculate the MSE for the every epoch  \n",
        "        error_output=np.array(expected)-np.array(output)\n",
        "        cost=(np.sum(error_output*error_output,axis=0)/len(error_output))\n",
        "        Etotal=np.sum(cost)\n",
        "        \n",
        "\n",
        "        \n",
        "    \n",
        "        return Etotal,neu_output\n",
        "\n",
        "a=[0.0031,0.0034,0.0036,0.0042]\n",
        "b=[0.0042,0.0035,0.0029,0.0038]\n",
        "c=[0.0036,0.0036,0.0036,0.0040]\n",
        "xticks = ['Model 1','Model 2','Model 3','Model 4']\n",
        "\n",
        "plt.plot(xticks,a,label='Train Error')\n",
        "plt.plot(xticks,b,label='Validation Error')\n",
        "plt.plot(xticks,c,label='Test Error')\n",
        "plt.ylabel(\"MSE\")  # add X-axis label \n",
        "plt.xlabel(\"Model Complexity\") \n",
        "plt.legend()\n",
        "\n",
        "A1,output1=test(Train,Weights,archicture,sigmoid)\n",
        "A2,output2=test(Validation,Weights,archicture,sigmoid)\n",
        "A3,output3=test(Test,Weights,archicture,sigmoid)\n",
        "print('Training Error: ',A1)\n",
        "print('Validation Error: ',A2)\n",
        "print('Test Error: ',A3)\n",
        "\n",
        "def plot(data,output):\n",
        "    \n",
        "    plt.rcParams['figure.figsize'] = (20, 30)\n",
        "    # plt.rcParams['axes.facecolor']='w'\n",
        "    fig = plt.figure()\n",
        "    ax1 = fig.add_subplot(421)\n",
        "    ax2 = fig.add_subplot(422)\n",
        "    ax3 = fig.add_subplot(423)\n",
        "#     ax4 = fig.add_subplot(424, projection='3d')\n",
        "#     ax5 = fig.add_subplot(425, projection='3d')\n",
        "#     ax6 = fig.add_subplot(426, projection='3d')\n",
        "#     ax7 = fig.add_subplot(427, projection='3d')\n",
        "#     ax8 = fig.add_subplot(428, projection='3d')\n",
        "\n",
        "    axs=[ax1,ax2,ax3]\n",
        "    \n",
        "\n",
        "    for j in range(len(axs)):\n",
        "        axs[j].set_xlabel('X Label')\n",
        "        axs[j].set_ylabel('Y Label')\n",
        "        # axs[j].set_zlabel('Z Label') \n",
        "\n",
        "    for j in range(len(axs)):\n",
        "\n",
        "        for i in range (len(data)):\n",
        "\n",
        "            # Data for three-dimensional scattered points\n",
        "            r=j+1\n",
        "            l=r-1\n",
        "\n",
        "            zdata = output[i][l:r]\n",
        "            xdata = data[i][1:2]\n",
        "            ydata = data[i][2:3]\n",
        "            axs[j].scatter(xdata, zdata, color='r',edgecolors='black');\n",
        "\n",
        "\n",
        "    # plt.savefig('Figure/Neuron_output.png',bbox_inches = 'tight')\n",
        "\n",
        "plot(Train,output1)\n",
        "\n",
        "plot(Validation,output2)\n",
        "\n",
        "plot(Test,output3)\n",
        "def plot2(data,output):\n",
        "    \n",
        "    plt.rcParams['figure.figsize'] = (10, 10)\n",
        "    # plt.rcParams['axes.facecolor']='w'\n",
        "    fig = plt.figure()\n",
        "    ax1 = fig.add_subplot(111)\n",
        "\n",
        "\n",
        "    axs=[ax1]\n",
        "\n",
        "    for j in range(len(axs)):\n",
        "        axs[j].set_xlabel('X Label')\n",
        "        axs[j].set_ylabel('Y Label')\n",
        "        # axs[j].set_zlabel('Z Label') \n",
        "\n",
        "    for j in range(len(axs)):\n",
        "\n",
        "        for i in range (len(data)):\n",
        "\n",
        "            # Data for three-dimensional scattered points\n",
        "            r=j+1\n",
        "            l=r-1\n",
        "\n",
        "            zdata = output[i][l:r]\n",
        "            xdata = data[i][1:2]\n",
        "            ydata = data[i][2:3]\n",
        "            axs[j].scatter(xdata, zdata, color='r',edgecolors='black');\n",
        "\n",
        "\n",
        "    # plt.savefig('Figure/Neuron_output.png',bbox_inches = 'tight')\n"
      ],
      "metadata": {
        "id": "y1r2SxL21W6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Decision boundary\n",
        "def generate_values_boundary2(data,weights,archicture,ttl):\n",
        "   \n",
        "\n",
        "    \n",
        "    network_outputs=[]\n",
        "    for i in range(len(data)):\n",
        "\n",
        "        input_data=np.array(data[i][:2])\n",
        "#         network_outputs.append(input_data) \n",
        "#         print(input_data)\n",
        "        #Calculation of the weighted sum at the every layer of the neural network\n",
        "        for h in range(len(archicture)-1):\n",
        "\n",
        "            if h== len(archicture)-2:\n",
        "                output_vector=np.dot(weights[h],input_data)\n",
        "#                 print(output_vector)\n",
        "            else:\n",
        "                output_vector=perceptron(weights[h],input_data,sigmoid)\n",
        "#                 print(output_vector)\n",
        "            input_data=np.concatenate(([1],output_vector))\n",
        "            \n",
        "        network_outputs.append(output_vector[0])\n",
        "\n",
        "\n",
        "    z= np.array(network_outputs)\n",
        "#     print(data[:,3])  \n",
        "#     print(z)\n",
        "    plt.scatter(data[:,2],z,marker='D',color='#EAA358',edgecolors='black')\n",
        "    \n",
        "    \n",
        "    \n",
        "    print(ttl)\n",
        "    plt.xlabel(\"Target Output\")  # add X-axis label \n",
        "    plt.ylabel(\"Model Output\") \n",
        "#     plt.savefig(ttl+'.png',bbox_inches = 'tight')\n",
        "    fig = plt.figure(figsize=(8,6))\n",
        "    plt.show()\n",
        "\n",
        "generate_values_boundary2(Train,Weights,archicture,\"r\")\n",
        "\n",
        "generate_values_boundary2(Validation,Weights,archicture,\"r\")\n",
        "generate_values_boundary2(Test,Weights,archicture,\"r\")"
      ],
      "metadata": {
        "id": "cB4eHyy41XEy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}